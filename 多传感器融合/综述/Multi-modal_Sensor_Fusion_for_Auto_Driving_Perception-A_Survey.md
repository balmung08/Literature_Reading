### 多传感器融合感知综述
> **Multi-modal Sensor Fusion for Auto Driving Perception: A Survey**

#### 融合方式分类
本文将融合方式分为弱融合与强融合，强融合中又分为早期融合、深度融合、后期融合和不对称融合

#### 融合感知的常见任务
* 目标检测，包括二维目标检测与三维目标检测
* 语义分割，比如将地面分为可驾驶与不可驾驶部分或进行车道分割
* 目标分类，确认点云或图像的物体类型
* 深度补全与预测：给定点云与图像时预测每个图像像素到观察者的距离

#### 激光点云的场景种类
* 点云格式可以分为基于点、体素和2D映射
* 基于点的点云表示
    通常大多数激光雷达生成的点云就是这个格式，通常包含每个点的三维坐标信息与反射强度信息，部分雷达还可以测出点的与颜色信息、回波次数信息等，可以用于辨认障碍材料等其他用途。通常使用基于点的点云表示来提取特征会造成信息冗余与速度降低，故在进行下一步前通常将此格式的数据转换为体素或2D映射。近年有使用RCNN系的二阶段神经网络与图神经网络进行特征提取的方法，能够保证很高的准确度，但是速度虽然有提升但仍旧较慢
* 基于体素的点云表示
    体素可以理解为三维空间的像素，是立方体；点云的点之间是稀疏的，但体素是将像素内的空间完全占据了；点云体素化将点云离散化在空间上均匀的体素格，然后生成点云和它们对应的体素之间多对一的映射。近年的工作正在考虑点云的更合理离散化方式；另外，体素化后极大的降低了冗余度，3D稀疏卷积技术在本来就较快的速度基础上提升了准确度
* 基于2D映射的点云表示
    将激光雷达数据投影到图像空间；相机平面图(CPM)和鸟瞰图(BEV)；
    CPM是将每个点投影到相机坐标系，此时每个点都有自己在相机图像中的坐标，很方便与相机图像融合。不过在较远处点云较为稀疏，导致一部分图像没有对应的点云。另外，会丢失点云的深度信息；
    BEV是将每个点投影到俯视二维坐标系，会丢失点云的Z轴高度信息，不过更方便作路径规划等需要俯视图的任务

#### 融合方法
> 传统分类：数据级->特征级->目标级
* **强融合**
* * 早期融合：激光点云+原始图像/图像经过语义分割后产生的特征    
* * 深度融合：激光点云产生的特征+原始图像/图像经过语义分割后产生的特征
* * 后期融合：激光点云目标检测候选框+图像目标检测候选框(目标级融合)
* * 不对称融合：一个分支的目标级加另一个分支的数据级/特征级；一个分支占主导，另一个分支提供建议
* **弱融合**
* * 不直接进行分支间的融合，通常使用基于某种规则的方法利用一种模态中的数据作为监控信号指导另一种模态的交互；如使用图像在点云中产生一些特殊区域，随后直接使用特殊区域点云与原始点云进行融合
* **另外还有复合融合方法，即在整个模型框架中不止有一种融合方法**

#### 未来可能发展方向
* 失准与信息丢失
* * CPM会产生噪声使得像素对齐不够精准
* * BEV会丢失大量信息
* 更合理的融合操作
* * 目前的融合方式都比较直观，可能无法融合分布差异较大的数据
* 利用更多潜在有用信息
* * 更深入的利用时间、上下文与空间信息
* 表示学习的自我监督
* * 交叉数据模式的数据直接可以自我监督学习，包括预训练、fine-tuning和对比学习等
* 传感器固有问题-数据域偏差
* * 不同原理传感器提取的数据有严重的领域特征，如机械lidar和固态lidar会有差距；另外，即使是相同传感器，数据也可能有区域偏见，可能导致泛化失败，数据集的重复利用率下降
* * 传感器固有问题-数据解析冲突
* * 传感器之间的分辨率等固有性质不一样；如激光雷达空间密度明显低于图像时，会因为找不到对应关系导致信息丢失，导致模型由多模态平等变为其中一种主导，导致信息不平衡